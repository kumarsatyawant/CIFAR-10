{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGG-19.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"5zKo-xR9Nfr7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4317063c-92f9-404c-b9e1-012a1375f546","executionInfo":{"status":"ok","timestamp":1639233661650,"user_tz":-540,"elapsed":25987,"user":{"displayName":"Kumar Satyawant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06015641520573185985"}}},"source":["from google.colab import files,drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"-TXk5bA5ViM3"},"source":["import torch\n","import math\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","import time\n","import os\n","import torch.backends.cudnn as cudnn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBknFU3BWLfc"},"source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n","start_time = time.time()\n","batch_size = 8\n","learning_rate = 0.001\n","root_dir = 'gdrive/My Drive/model/cifar10/'\n","default_directory = 'gdrive/My Drive/model/save_models'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"361B8aJrXNOw"},"source":["# Data Augmentation\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n","    transforms.RandomHorizontalFlip(),                  # right and left flip\n","    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n","    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n","                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n","])\n","\n","# transforms.RandomRotation(degrees=180), #my addition\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n","    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n","                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zVtlTGo5XTIu","outputId":"73039c02-f824-4a4b-e41e-526d307acb2f","executionInfo":{"status":"ok","timestamp":1639233677591,"user_tz":-540,"elapsed":9897,"user":{"displayName":"Kumar Satyawant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06015641520573185985"}}},"source":["# automatically download\n","train_dataset = datasets.CIFAR10(root=root_dir,\n","                                 train=True,\n","                                 transform=transform_train,\n","                                 download=True)\n","\n","test_dataset = datasets.CIFAR10(root=root_dir,\n","                                train=False,\n","                                transform=transform_test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E-G19vW4XVx_","outputId":"a9aad154-0ab2-4db9-e650-0bc71b9539a7","executionInfo":{"status":"ok","timestamp":1639233677593,"user_tz":-540,"elapsed":39,"user":{"displayName":"Kumar Satyawant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06015641520573185985"}}},"source":["train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n","                                           num_workers=4)           # CPU loader number\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n","                                          num_workers=4)            # CPU loader number"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","metadata":{"id":"7cRHrVjWln5U"},"source":["class VGG(nn.Module):\n","\n","    def __init__(self, num_classes=10):\n","        super(VGG, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(inplace=True),\n","            \n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(inplace=True),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(p=0.6),\n","            nn.Linear(512, 512),\n","            nn.LeakyReLU(True),\n","            nn.Dropout(p=0.6),\n","            nn.Linear(512, 512),\n","            nn.LeakyReLU(True),\n","            nn.Linear(512, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","model = VGG()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTH8pYuQYus2"},"source":["optimizer = optim.SGD(model.parameters(), learning_rate,\n","                                momentum=0.9,\n","                                weight_decay=1e-4,\n","                                nesterov=True)\n","criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cle2IBw0YuwH","outputId":"98c2146a-aaab-4ab6-a1d4-46ecc5104f17","executionInfo":{"status":"ok","timestamp":1639233688040,"user_tz":-540,"elapsed":10470,"user":{"displayName":"Kumar Satyawant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06015641520573185985"}}},"source":["if torch.cuda.device_count() > 0:\n","    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n","    model = nn.DataParallel(model).cuda()\n","    cudnn.benchmark = True\n","else:\n","    print(\"USE ONLY CPU!\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["USE 1 GPUs!\n"]}]},{"cell_type":"code","metadata":{"id":"Mh9GEElY_gRq"},"source":["train_losses = []\n","test_losses = []\n","\n","train_accuracy = []\n","test_accuracy = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ntsS_yVBY_V2"},"source":["def train(epoch):\n","    model.train()\n","    train_loss = 0 \n","    total = 0\n","    correct = 0\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        if torch.cuda.is_available():\n","            data, target = Variable(data.cuda()), Variable(target.cuda())\n","        else:\n","            data, target = Variable(data), Variable(target)\n","\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = torch.max(output.data, 1)\n","\n","        total += target.size(0)\n","        correct += predicted.eq(target.data).cpu().sum()\n","        if batch_idx % 10 == 0:\n","            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n","                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n","            \n","    train_losses.append(train_loss / (batch_idx + 1))\n","    train_accuracy.append(100. * correct / total)\n","\n","\n","def test():\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (data, target) in enumerate(test_loader):\n","        if torch.cuda.is_available():\n","            data, target = Variable(data.cuda()), Variable(target.cuda())\n","        else:\n","            data, target = Variable(data), Variable(target)\n","\n","        outputs = model(data)\n","        loss = criterion(outputs, target)\n","\n","        test_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += target.size(0)\n","        correct += predicted.eq(target.data).cpu().sum()\n","    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n","          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n","    \n","    test_losses.append(test_loss / (batch_idx + 1))\n","    test_accuracy.append(100. * correct / total)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMHgEk56Y_Y_"},"source":["def save_checkpoint(directory, state, filename='latest.tar.gz'):\n","\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","    model_filename = os.path.join(directory, filename)\n","    torch.save(state, model_filename)\n","    print(\"=> saving checkpoint\")\n","\n","def load_checkpoint(directory, filename='latest.tar.gz'):\n","\n","    model_filename = os.path.join(directory, filename)\n","    if os.path.exists(model_filename):\n","        print(\"=> loading checkpoint\")\n","        state = torch.load(model_filename)\n","        return state\n","    else:\n","        return None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xlcgRL98ZVXW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639233690540,"user_tz":-540,"elapsed":2511,"user":{"displayName":"Kumar Satyawant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06015641520573185985"}},"outputId":"2d059ae8-8fad-4a89-9b94-ed798677036c"},"source":["start_epoch = 0\n","\n","checkpoint = load_checkpoint(default_directory)\n","if not checkpoint:\n","    pass\n","else:\n","    start_epoch = checkpoint['epoch'] + 1\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","\n","    train_losses = checkpoint['train_loss']\n","    test_losses = checkpoint['test_loss']\n","    train_accuracy = checkpoint['train_accuracy']\n","    test_accuracy = checkpoint['test_accuracy']"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=> loading checkpoint\n"]}]},{"cell_type":"code","metadata":{"id":"xn6BRInCY_b2"},"source":["for epoch in range(start_epoch, 190):\n","\n","    if epoch < 50:\n","        lr = learning_rate\n","    elif epoch < 90:\n","        lr = learning_rate * 0.1\n","    elif epoch < 110:\n","        lr = learning_rate * 0.01\n","    else:\n","        lr = learning_rate * 0.001\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","    train(epoch)\n","    save_checkpoint(default_directory, {\n","        'epoch': epoch,\n","        'model': model,\n","        'state_dict': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","        'train_loss': train_losses,\n","        'test_loss': test_losses,\n","        'train_accuracy': train_accuracy,\n","        'test_accuracy': test_accuracy\n","    })\n","    test()  \n","\n","now = time.gmtime(time.time() - start_time)\n","print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"],"execution_count":null,"outputs":[]}]}